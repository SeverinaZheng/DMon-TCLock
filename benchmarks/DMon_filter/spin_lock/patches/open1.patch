diff -ruN SynCord-linux-base/fs/ext4/mballoc.c SynCord-linux-destination/fs/ext4/mballoc.c
--- SynCord-linux-base/fs/ext4/mballoc.c	2023-08-29 15:29:11.578283468 +0000
+++ SynCord-linux-destination/fs/ext4/mballoc.c	2023-08-31 09:47:51.756588968 +0000
@@ -17,6 +17,8 @@
 #include <linux/nospec.h>
 #include <linux/backing-dev.h>
 #include <trace/events/ext4.h>
+#include <linux/my_bpf_spin_lock.h>
+#include <linux/my_bpf_spin_lock.h>
 
 #ifdef CONFIG_EXT4_DEBUG
 ushort ext4_mballoc_debug __read_mostly;
@@ -3163,13 +3165,14 @@
 	/* check we don't cross already preallocated blocks */
 	rcu_read_lock();
 	list_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {
+		extern int num_policy;
 		ext4_lblk_t pa_end;
 
 		if (pa->pa_deleted)
 			continue;
-		spin_lock(&pa->pa_lock);
+		my_bpf_spin_lock(&pa->pa_lock, num_policy);
 		if (pa->pa_deleted) {
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			continue;
 		}
 
@@ -3182,7 +3185,7 @@
 
 		/* skip PAs this normalized request doesn't overlap with */
 		if (pa->pa_lstart >= end || pa_end <= start) {
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			continue;
 		}
 		BUG_ON(pa->pa_lstart <= start && pa_end >= end);
@@ -3195,7 +3198,7 @@
 			BUG_ON(pa->pa_lstart > end);
 			end = pa->pa_lstart;
 		}
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 	}
 	rcu_read_unlock();
 	size = end - start;
@@ -3203,15 +3206,16 @@
 	/* XXX: extra loop to check we really don't overlap preallocations */
 	rcu_read_lock();
 	list_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {
+		extern int num_policy;
 		ext4_lblk_t pa_end;
 
-		spin_lock(&pa->pa_lock);
+		my_bpf_spin_lock(&pa->pa_lock, num_policy);
 		if (pa->pa_deleted == 0) {
 			pa_end = pa->pa_lstart + EXT4_C2B(EXT4_SB(ac->ac_sb),
 							  pa->pa_len);
 			BUG_ON(!(start >= pa_end || end <= pa->pa_lstart));
 		}
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 	}
 	rcu_read_unlock();
 
@@ -3414,6 +3418,7 @@
 	/* first, try per-file preallocation */
 	rcu_read_lock();
 	list_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {
+		extern int num_policy;
 
 		/* all fields in this condition don't change,
 		 * so we can skip locking for them */
@@ -3429,16 +3434,16 @@
 			continue;
 
 		/* found preallocated blocks, use them */
-		spin_lock(&pa->pa_lock);
+		my_bpf_spin_lock(&pa->pa_lock, num_policy);
 		if (pa->pa_deleted == 0 && pa->pa_free) {
 			atomic_inc(&pa->pa_count);
 			ext4_mb_use_inode_pa(ac, pa);
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			ac->ac_criteria = 10;
 			rcu_read_unlock();
 			return 1;
 		}
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 	}
 	rcu_read_unlock();
 
@@ -3464,14 +3469,15 @@
 		rcu_read_lock();
 		list_for_each_entry_rcu(pa, &lg->lg_prealloc_list[i],
 					pa_inode_list) {
-			spin_lock(&pa->pa_lock);
+			extern int num_policy;
+			my_bpf_spin_lock(&pa->pa_lock, num_policy);
 			if (pa->pa_deleted == 0 &&
 					pa->pa_free >= ac->ac_o_ex.fe_len) {
 
 				cpa = ext4_mb_check_group_pa(goal_block,
 								pa, cpa);
 			}
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 		}
 		rcu_read_unlock();
 	}
@@ -3533,12 +3539,13 @@
 	 * is dropping preallocation
 	 */
 	list_for_each(cur, &grp->bb_prealloc_list) {
+		extern int num_policy;
 		pa = list_entry(cur, struct ext4_prealloc_space, pa_group_list);
-		spin_lock(&pa->pa_lock);
+		my_bpf_spin_lock(&pa->pa_lock, num_policy);
 		ext4_get_group_no_and_offset(sb, pa->pa_pstart,
 					     &groupnr, &start);
 		len = pa->pa_len;
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 		if (unlikely(len == 0))
 			continue;
 		BUG_ON(groupnr != group);
@@ -3565,23 +3572,24 @@
 static void ext4_mb_put_pa(struct ext4_allocation_context *ac,
 			struct super_block *sb, struct ext4_prealloc_space *pa)
 {
+	extern int num_policy;
 	ext4_group_t grp;
 	ext4_fsblk_t grp_blk;
 
 	/* in this short window concurrent discard can set pa_deleted */
-	spin_lock(&pa->pa_lock);
+	my_bpf_spin_lock(&pa->pa_lock, num_policy);
 	if (!atomic_dec_and_test(&pa->pa_count) || pa->pa_free != 0) {
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 		return;
 	}
 
 	if (pa->pa_deleted == 1) {
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 		return;
 	}
 
 	pa->pa_deleted = 1;
-	spin_unlock(&pa->pa_lock);
+	my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 
 	grp_blk = pa->pa_pstart;
 	/*
@@ -3916,14 +3924,15 @@
 	ext4_lock_group(sb, group);
 	list_for_each_entry_safe(pa, tmp,
 				&grp->bb_prealloc_list, pa_group_list) {
-		spin_lock(&pa->pa_lock);
+		extern int num_policy;
+		my_bpf_spin_lock(&pa->pa_lock, num_policy);
 		if (atomic_read(&pa->pa_count)) {
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			busy = 1;
 			continue;
 		}
 		if (pa->pa_deleted) {
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			continue;
 		}
 
@@ -3933,7 +3942,7 @@
 		/* we can trust pa_free ... */
 		free += pa->pa_free;
 
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 
 		list_del(&pa->pa_group_list);
 		list_add(&pa->u.pa_tmp_list, &list);
@@ -4011,15 +4020,18 @@
 	/* first, collect all pa's in the inode */
 	spin_lock(&ei->i_prealloc_lock);
 	while (!list_empty(&ei->i_prealloc_list)) {
+		extern int num_policy;
+		extern int num_policy;
 		pa = list_entry(ei->i_prealloc_list.next,
 				struct ext4_prealloc_space, pa_inode_list);
 		BUG_ON(pa->pa_obj_lock != &ei->i_prealloc_lock);
-		spin_lock(&pa->pa_lock);
+		my_bpf_spin_lock(&pa->pa_lock, num_policy);
 		if (atomic_read(&pa->pa_count)) {
+			extern int num_policy;
 			/* this shouldn't happen often - nobody should
 			 * use preallocation while we're discarding it */
-			spin_unlock(&pa->pa_lock);
-			spin_unlock(&ei->i_prealloc_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
+			my_bpf_spin_unlock(&ei->i_prealloc_lock, num_policy);
 			ext4_msg(sb, KERN_ERR,
 				 "uh-oh! used pa while discarding");
 			WARN_ON(1);
@@ -4029,15 +4041,15 @@
 		}
 		if (pa->pa_deleted == 0) {
 			pa->pa_deleted = 1;
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			list_del_rcu(&pa->pa_inode_list);
 			list_add(&pa->u.pa_tmp_list, &list);
 			continue;
 		}
 
 		/* someone is deleting pa right now */
-		spin_unlock(&pa->pa_lock);
-		spin_unlock(&ei->i_prealloc_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
+		my_bpf_spin_unlock(&ei->i_prealloc_lock, num_policy);
 
 		/* we have to wait here because pa_deleted
 		 * doesn't mean pa is already unlinked from
@@ -4130,12 +4142,13 @@
 		struct list_head *cur;
 		ext4_lock_group(sb, i);
 		list_for_each(cur, &grp->bb_prealloc_list) {
+			extern int num_policy;
 			pa = list_entry(cur, struct ext4_prealloc_space,
 					pa_group_list);
-			spin_lock(&pa->pa_lock);
+			my_bpf_spin_lock(&pa->pa_lock, num_policy);
 			ext4_get_group_no_and_offset(sb, pa->pa_pstart,
 						     NULL, &start);
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			printk(KERN_ERR "PA:%u:%d:%u \n", i,
 			       start, pa->pa_len);
 		}
@@ -4281,18 +4294,19 @@
 	spin_lock(&lg->lg_prealloc_lock);
 	list_for_each_entry_rcu(pa, &lg->lg_prealloc_list[order],
 						pa_inode_list) {
-		spin_lock(&pa->pa_lock);
+		extern int num_policy;
+		my_bpf_spin_lock(&pa->pa_lock, num_policy);
 		if (atomic_read(&pa->pa_count)) {
 			/*
 			 * This is the pa that we just used
 			 * for block allocation. So don't
 			 * free that
 			 */
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			continue;
 		}
 		if (pa->pa_deleted) {
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 			continue;
 		}
 		/* only lg prealloc space */
@@ -4300,7 +4314,7 @@
 
 		/* seems this one can be freed ... */
 		pa->pa_deleted = 1;
-		spin_unlock(&pa->pa_lock);
+		my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 
 		list_del_rcu(&pa->pa_inode_list);
 		list_add(&pa->u.pa_tmp_list, &discard_list);
@@ -4364,9 +4378,10 @@
 	spin_lock(&lg->lg_prealloc_lock);
 	list_for_each_entry_rcu(tmp_pa, &lg->lg_prealloc_list[order],
 						pa_inode_list) {
-		spin_lock(&tmp_pa->pa_lock);
+		extern int num_policy;
+		my_bpf_spin_lock(&tmp_pa->pa_lock, num_policy);
 		if (tmp_pa->pa_deleted) {
-			spin_unlock(&tmp_pa->pa_lock);
+			my_bpf_spin_unlock(&tmp_pa->pa_lock, num_policy);
 			continue;
 		}
 		if (!added && pa->pa_free < tmp_pa->pa_free) {
@@ -4379,7 +4394,7 @@
 			 * number of entries in the list
 			 */
 		}
-		spin_unlock(&tmp_pa->pa_lock);
+		my_bpf_spin_unlock(&tmp_pa->pa_lock, num_policy);
 		lg_prealloc_count++;
 	}
 	if (!added)
@@ -4405,13 +4420,14 @@
 	struct ext4_prealloc_space *pa = ac->ac_pa;
 	if (pa) {
 		if (pa->pa_type == MB_GROUP_PA) {
+			extern int num_policy;
 			/* see comment in ext4_mb_use_group_pa() */
-			spin_lock(&pa->pa_lock);
+			my_bpf_spin_lock(&pa->pa_lock, num_policy);
 			pa->pa_pstart += EXT4_C2B(sbi, ac->ac_b_ex.fe_len);
 			pa->pa_lstart += EXT4_C2B(sbi, ac->ac_b_ex.fe_len);
 			pa->pa_free -= ac->ac_b_ex.fe_len;
 			pa->pa_len -= ac->ac_b_ex.fe_len;
-			spin_unlock(&pa->pa_lock);
+			my_bpf_spin_unlock(&pa->pa_lock, num_policy);
 		}
 	}
 	if (pa) {
diff -ruN SynCord-linux-base/fs/file.c SynCord-linux-destination/fs/file.c
--- SynCord-linux-base/fs/file.c	2023-08-29 15:29:11.578283468 +0000
+++ SynCord-linux-destination/fs/file.c	2023-08-31 09:44:52.701890338 +0000
@@ -18,6 +18,7 @@
 #include <linux/bitops.h>
 #include <linux/spinlock.h>
 #include <linux/rcupdate.h>
+#include <linux/my_bpf_spin_lock.h>
 
 unsigned int sysctl_nr_open __read_mostly = 1024*1024;
 unsigned int sysctl_nr_open_min = BITS_PER_LONG;
@@ -149,9 +150,10 @@
 	__releases(files->file_lock)
 	__acquires(files->file_lock)
 {
+	extern int num_policy;
 	struct fdtable *new_fdt, *cur_fdt;
 
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	new_fdt = alloc_fdtable(nr);
 
 	/* make sure all __fd_install() have seen resize_in_progress
@@ -160,7 +162,7 @@
 	if (atomic_read(&files->count) > 1)
 		synchronize_rcu();
 
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	if (!new_fdt)
 		return -ENOMEM;
 	/*
@@ -209,10 +211,11 @@
 		return -EMFILE;
 
 	if (unlikely(files->resize_in_progress)) {
-		spin_unlock(&files->file_lock);
+		extern int num_policy;
+		my_bpf_spin_unlock(&files->file_lock, num_policy);
 		expanded = 1;
 		wait_event(files->resize_wait, !files->resize_in_progress);
-		spin_lock(&files->file_lock);
+		my_bpf_spin_lock(&files->file_lock, num_policy);
 		goto repeat;
 	}
 
@@ -271,6 +274,8 @@
  */
 struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 {
+	extern int num_policy;
+	extern int num_policy;
 	struct files_struct *newf;
 	struct file **old_fds, **new_fds;
 	unsigned int open_files, i;
@@ -294,7 +299,7 @@
 	new_fdt->full_fds_bits = newf->full_fds_bits_init;
 	new_fdt->fd = &newf->fd_array[0];
 
-	spin_lock(&oldf->file_lock);
+	my_bpf_spin_lock(&oldf->file_lock, num_policy);
 	old_fdt = files_fdtable(oldf);
 	open_files = count_open_files(old_fdt);
 
@@ -302,7 +307,8 @@
 	 * Check whether we need to allocate a larger fd array and fd set.
 	 */
 	while (unlikely(open_files > new_fdt->max_fds)) {
-		spin_unlock(&oldf->file_lock);
+		extern int num_policy;
+		my_bpf_spin_unlock(&oldf->file_lock, num_policy);
 
 		if (new_fdt != &newf->fdtab)
 			__free_fdtable(new_fdt);
@@ -325,7 +331,7 @@
 		 * who knows it may have a new bigger fd table. We need
 		 * the latest pointer.
 		 */
-		spin_lock(&oldf->file_lock);
+		my_bpf_spin_lock(&oldf->file_lock, num_policy);
 		old_fdt = files_fdtable(oldf);
 		open_files = count_open_files(old_fdt);
 	}
@@ -350,7 +356,7 @@
 		}
 		rcu_assign_pointer(*new_fds++, f);
 	}
-	spin_unlock(&oldf->file_lock);
+	my_bpf_spin_unlock(&oldf->file_lock, num_policy);
 
 	/* clear the remainder */
 	memset(new_fds, 0, (new_fdt->max_fds - open_files) * sizeof(struct file *));
@@ -480,11 +486,12 @@
 int __alloc_fd(struct files_struct *files,
 	       unsigned start, unsigned end, unsigned flags)
 {
+	extern int num_policy;
 	unsigned int fd;
 	int error;
 	struct fdtable *fdt;
 
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 repeat:
 	fdt = files_fdtable(files);
 	fd = start;
@@ -531,7 +538,7 @@
 #endif
 
 out:
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	return error;
 }
 
@@ -556,10 +563,11 @@
 
 void put_unused_fd(unsigned int fd)
 {
+	extern int num_policy;
 	struct files_struct *files = current->files;
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	__put_unused_fd(files, fd);
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 }
 
 EXPORT_SYMBOL(put_unused_fd);
@@ -592,12 +600,13 @@
 	rcu_read_lock_sched();
 
 	if (unlikely(files->resize_in_progress)) {
+		extern int num_policy;
 		rcu_read_unlock_sched();
-		spin_lock(&files->file_lock);
+		my_bpf_spin_lock(&files->file_lock, num_policy);
 		fdt = files_fdtable(files);
 		BUG_ON(fdt->fd[fd] != NULL);
 		rcu_assign_pointer(fdt->fd[fd], file);
-		spin_unlock(&files->file_lock);
+		my_bpf_spin_unlock(&files->file_lock, num_policy);
 		return;
 	}
 	/* coupled with smp_wmb() in expand_fdtable() */
@@ -620,10 +629,11 @@
  */
 int __close_fd(struct files_struct *files, unsigned fd)
 {
+	extern int num_policy;
 	struct file *file;
 	struct fdtable *fdt;
 
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	fdt = files_fdtable(files);
 	if (fd >= fdt->max_fds)
 		goto out_unlock;
@@ -632,11 +642,11 @@
 		goto out_unlock;
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	__put_unused_fd(files, fd);
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	return filp_close(file, files);
 
 out_unlock:
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	return -EBADF;
 }
 EXPORT_SYMBOL(__close_fd); /* for ksys_close() */
@@ -646,11 +656,12 @@
  */
 int __close_fd_get_file(unsigned int fd, struct file **res)
 {
+	extern int num_policy;
 	struct files_struct *files = current->files;
 	struct file *file;
 	struct fdtable *fdt;
 
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	fdt = files_fdtable(files);
 	if (fd >= fdt->max_fds)
 		goto out_unlock;
@@ -659,24 +670,26 @@
 		goto out_unlock;
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	__put_unused_fd(files, fd);
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	get_file(file);
 	*res = file;
 	return filp_close(file, files);
 
 out_unlock:
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	*res = NULL;
 	return -ENOENT;
 }
 
 void do_close_on_exec(struct files_struct *files)
 {
+	extern int num_policy;
+	extern int num_policy;
 	unsigned i;
 	struct fdtable *fdt;
 
 	/* exec unshares first */
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	for (i = 0; ; i++) {
 		unsigned long set;
 		unsigned fd = i * BITS_PER_LONG;
@@ -688,6 +701,7 @@
 			continue;
 		fdt->close_on_exec[i] = 0;
 		for ( ; set ; fd++, set >>= 1) {
+			extern int num_policy;
 			struct file *file;
 			if (!(set & 1))
 				continue;
@@ -696,14 +710,14 @@
 				continue;
 			rcu_assign_pointer(fdt->fd[fd], NULL);
 			__put_unused_fd(files, fd);
-			spin_unlock(&files->file_lock);
+			my_bpf_spin_unlock(&files->file_lock, num_policy);
 			filp_close(file, files);
 			cond_resched();
-			spin_lock(&files->file_lock);
+			my_bpf_spin_lock(&files->file_lock, num_policy);
 		}
 
 	}
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 }
 
 static struct file *__fget(unsigned int fd, fmode_t mask, unsigned int refs)
@@ -817,15 +831,16 @@
 
 void set_close_on_exec(unsigned int fd, int flag)
 {
+	extern int num_policy;
 	struct files_struct *files = current->files;
 	struct fdtable *fdt;
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	fdt = files_fdtable(files);
 	if (flag)
 		__set_close_on_exec(fd, fdt);
 	else
 		__clear_close_on_exec(fd, fdt);
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 }
 
 bool get_close_on_exec(unsigned int fd)
@@ -844,6 +859,7 @@
 	struct file *file, unsigned fd, unsigned flags)
 __releases(&files->file_lock)
 {
+	extern int num_policy;
 	struct file *tofree;
 	struct fdtable *fdt;
 
@@ -872,7 +888,7 @@
 		__set_close_on_exec(fd, fdt);
 	else
 		__clear_close_on_exec(fd, fdt);
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 
 	if (tofree)
 		filp_close(tofree, files);
@@ -880,12 +896,13 @@
 	return fd;
 
 Ebusy:
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	return -EBUSY;
 }
 
 int replace_fd(unsigned fd, struct file *file, unsigned flags)
 {
+	extern int num_policy;
 	int err;
 	struct files_struct *files = current->files;
 
@@ -895,7 +912,7 @@
 	if (fd >= rlimit(RLIMIT_NOFILE))
 		return -EBADF;
 
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	err = expand_files(files, fd);
 	if (unlikely(err < 0))
 		goto out_unlock;
@@ -908,6 +925,7 @@
 
 static int ksys_dup3(unsigned int oldfd, unsigned int newfd, int flags)
 {
+	extern int num_policy;
 	int err = -EBADF;
 	struct file *file;
 	struct files_struct *files = current->files;
@@ -921,7 +939,7 @@
 	if (newfd >= rlimit(RLIMIT_NOFILE))
 		return -EBADF;
 
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	err = expand_files(files, newfd);
 	file = fcheck(oldfd);
 	if (unlikely(!file))
@@ -997,11 +1015,12 @@
 		int (*f)(const void *, struct file *, unsigned),
 		const void *p)
 {
+	extern int num_policy;
 	struct fdtable *fdt;
 	int res = 0;
 	if (!files)
 		return 0;
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	for (fdt = files_fdtable(files); n < fdt->max_fds; n++) {
 		struct file *file;
 		file = rcu_dereference_check_fdtable(files, fdt->fd[n]);
@@ -1011,7 +1030,7 @@
 		if (res)
 			break;
 	}
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	return res;
 }
 EXPORT_SYMBOL(iterate_fd);
diff -ruN SynCord-linux-base/fs/locks.c SynCord-linux-destination/fs/locks.c
--- SynCord-linux-base/fs/locks.c	2023-08-29 15:29:11.578283468 +0000
+++ SynCord-linux-destination/fs/locks.c	2023-08-31 09:44:52.441886957 +0000
@@ -168,6 +168,7 @@
 #include <linux/pid_namespace.h>
 #include <linux/hashtable.h>
 #include <linux/percpu.h>
+#include <linux/my_bpf_spin_lock.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/filelock.h>
@@ -2504,14 +2505,15 @@
 	 */
 	if (!error && file_lock->fl_type != F_UNLCK &&
 	    !(file_lock->fl_flags & FL_OFDLCK)) {
+		extern int num_policy;
 		/*
 		 * We need that spin_lock here - it prevents reordering between
 		 * update of i_flctx->flc_posix and check for it done in
 		 * close(). rcu_read_lock() wouldn't do.
 		 */
-		spin_lock(&current->files->file_lock);
+		my_bpf_spin_lock(&current->files->file_lock, num_policy);
 		f = fcheck(fd);
-		spin_unlock(&current->files->file_lock);
+		my_bpf_spin_unlock(&current->files->file_lock, num_policy);
 		if (f != filp) {
 			file_lock->fl_type = F_UNLCK;
 			error = do_lock_file_wait(filp, cmd, file_lock);
@@ -2635,14 +2637,15 @@
 	 */
 	if (!error && file_lock->fl_type != F_UNLCK &&
 	    !(file_lock->fl_flags & FL_OFDLCK)) {
+		extern int num_policy;
 		/*
 		 * We need that spin_lock here - it prevents reordering between
 		 * update of i_flctx->flc_posix and check for it done in
 		 * close(). rcu_read_lock() wouldn't do.
 		 */
-		spin_lock(&current->files->file_lock);
+		my_bpf_spin_lock(&current->files->file_lock, num_policy);
 		f = fcheck(fd);
-		spin_unlock(&current->files->file_lock);
+		my_bpf_spin_unlock(&current->files->file_lock, num_policy);
 		if (f != filp) {
 			file_lock->fl_type = F_UNLCK;
 			error = do_lock_file_wait(filp, cmd, file_lock);
diff -ruN SynCord-linux-base/fs/proc/fd.c SynCord-linux-destination/fs/proc/fd.c
--- SynCord-linux-base/fs/proc/fd.c	2023-08-29 15:29:11.578283468 +0000
+++ SynCord-linux-destination/fs/proc/fd.c	2023-08-31 09:44:52.613889193 +0000
@@ -10,6 +10,7 @@
 #include <linux/file.h>
 #include <linux/seq_file.h>
 #include <linux/fs.h>
+#include <linux/my_bpf_spin_lock.h>
 
 #include <linux/proc_fs.h>
 
@@ -32,9 +33,10 @@
 	put_task_struct(task);
 
 	if (files) {
+		extern int num_policy;
 		unsigned int fd = proc_fd(m->private);
 
-		spin_lock(&files->file_lock);
+		my_bpf_spin_lock(&files->file_lock, num_policy);
 		file = fcheck_files(files, fd);
 		if (file) {
 			struct fdtable *fdt = files_fdtable(files);
@@ -46,7 +48,7 @@
 			get_file(file);
 			ret = 0;
 		}
-		spin_unlock(&files->file_lock);
+		my_bpf_spin_unlock(&files->file_lock, num_policy);
 		put_files_struct(files);
 	}
 
@@ -157,17 +159,18 @@
 	}
 
 	if (files) {
+		extern int num_policy;
 		unsigned int fd = proc_fd(d_inode(dentry));
 		struct file *fd_file;
 
-		spin_lock(&files->file_lock);
+		my_bpf_spin_lock(&files->file_lock, num_policy);
 		fd_file = fcheck_files(files, fd);
 		if (fd_file) {
 			*path = fd_file->f_path;
 			path_get(&fd_file->f_path);
 			ret = 0;
 		}
-		spin_unlock(&files->file_lock);
+		my_bpf_spin_unlock(&files->file_lock, num_policy);
 		put_files_struct(files);
 	}
 
diff -ruN SynCord-linux-base/include/linux/my_bpf_spin_lock.h SynCord-linux-destination/include/linux/my_bpf_spin_lock.h
--- SynCord-linux-base/include/linux/my_bpf_spin_lock.h	1970-01-01 00:00:00.000000000 +0000
+++ SynCord-linux-destination/include/linux/my_bpf_spin_lock.h	2023-08-31 09:48:51.772133943 +0000
@@ -0,0 +1,41 @@
+#ifndef MY_BPF_SPIN_H
+#define MY_BPF_SPIN_H
+#define bpf_arch_spin_lock(l,policy) bpf_queued_spin_lock(l,policy) 
+static inline void bpf_do_raw_spin_lock(raw_spinlock_t *lock, int policy) __acquires(lock)
+{
+	__acquire(lock);
+	bpf_arch_spin_lock(&lock->raw_lock,policy);
+	mmiowb_spin_lock();
+}
+static inline void bpf___raw_spin_lock(raw_spinlock_t *lock, int policy)
+{
+	preempt_disable();
+	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	bpf_do_raw_spin_lock(lock,policy);
+}
+#define bpf__raw_spin_lock(lock,policy) bpf___raw_spin_lock(lock,policy) 
+#define bpf_raw_spin_lock(lock,policy) bpf__raw_spin_lock(lock,policy) 
+static __always_inline void my_bpf_spin_lock(spinlock_t *lock, int policy)
+{
+	bpf_raw_spin_lock(&lock->rlock,policy);
+}
+#define bpf_arch_spin_unlock(l,policy) bpf_queued_spin_unlock(l,policy) 
+static inline void bpf_do_raw_spin_unlock(raw_spinlock_t *lock, int policy) __releases(lock)
+{
+	mmiowb_spin_unlock();
+	bpf_arch_spin_unlock(&lock->raw_lock,policy);
+	__release(lock);
+}
+static inline void bpf___raw_spin_unlock(raw_spinlock_t *lock, int policy)
+{
+	spin_release(&lock->dep_map, 1, _RET_IP_);
+	bpf_do_raw_spin_unlock(lock,policy);
+	preempt_enable();
+}
+#define bpf__raw_spin_unlock(lock,policy) bpf___raw_spin_unlock(lock,policy) 
+#define bpf_raw_spin_unlock(lock,policy) bpf__raw_spin_unlock(lock,policy) 
+static __always_inline void my_bpf_spin_unlock(spinlock_t *lock, int policy)
+{
+	bpf_raw_spin_unlock(&lock->rlock,policy);
+}
+#endif
\ No newline at end of file
diff -ruN SynCord-linux-base/kernel/bpf/inode.c SynCord-linux-destination/kernel/bpf/inode.c
--- SynCord-linux-base/kernel/bpf/inode.c	2023-08-29 15:29:11.582283462 +0000
+++ SynCord-linux-destination/kernel/bpf/inode.c	2023-08-31 09:43:56.837169532 +0000
@@ -701,3 +701,76 @@
 	return ret;
 }
 fs_initcall(bpf_init);
+
+#include "kpatch-macros.h"
+#define MAX_POLICY 5
+
+static inline __u64 ptr_to_u64(const void *ptr)
+{
+    return (__u64) (unsigned long) ptr;
+}
+
+static void *get_pinned_bpf_obj(const char *pathname){
+	struct inode *inode;
+	struct path path;
+	void *raw;
+	int ret;
+
+	/* Let's get BPF prog 1 */
+	ret = kern_path(pathname, LOOKUP_FOLLOW, &path);
+	if (ret){
+		printk("[syncord] %s failed\n", pathname);
+		return ERR_PTR(ret);
+	}
+
+	inode = d_backing_inode(path.dentry);
+	ret = inode_permission(inode, ACC_MODE(2));
+	if(ret){
+		printk("[syncord] perm error\n");
+		path_put(&path);
+		return ERR_PTR(ret);
+	}
+
+	raw = bpf_any_get(inode->i_private, BPF_TYPE_PROG);
+	if(!IS_ERR(raw)){
+		touch_atime(&path);
+	}
+	else{
+		printk("[syncord] raw error\n");
+		path_put(&path);
+		return ERR_PTR(ret);
+	}
+
+	path_put(&path);
+	return raw;
+}
+
+static int pre_patch_callback(patch_object *obj)
+{
+	extern int num_policy;
+	extern void *bpf_prog_should_reorder[MAX_POLICY];
+
+	if(num_policy < 4)
+		num_policy++;
+	else
+		return -1;
+
+	bpf_prog_should_reorder[num_policy] = get_pinned_bpf_obj("/sys/fs/bpf/numa-grouping");
+	if(IS_ERR(bpf_prog_should_reorder[num_policy])){
+		printk("[syncord] bpf_policy failed\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+static void post_unpatch_callback(patch_object *obj) {
+	extern int num_policy;
+	extern void *bpf_prog_should_reorder[MAX_POLICY];
+
+	bpf_prog_should_reorder[num_policy] = NULL;
+	num_policy--;
+	klp_shadow_free_all(0, NULL);
+}
+KPATCH_PRE_PATCH_CALLBACK(pre_patch_callback);
+KPATCH_POST_UNPATCH_CALLBACK(post_unpatch_callback);
diff -ruN SynCord-linux-base/kernel/bpf/inode.c.rej SynCord-linux-destination/kernel/bpf/inode.c.rej
--- SynCord-linux-base/kernel/bpf/inode.c.rej	1970-01-01 00:00:00.000000000 +0000
+++ SynCord-linux-destination/kernel/bpf/inode.c.rej	2023-08-31 09:46:50.649192350 +0000
@@ -0,0 +1,79 @@
+--- kernel/bpf/inode.c	2023-08-16 13:14:17.681646915 +0000
++++ kernel/bpf/inode.c	2023-08-16 13:18:40.190660851 +0000
+@@ -701,3 +701,76 @@
+ 	return ret;
+ }
+ fs_initcall(bpf_init);
++
++#include "kpatch-macros.h"
++#define MAX_POLICY 5
++
++static inline __u64 ptr_to_u64(const void *ptr)
++{
++    return (__u64) (unsigned long) ptr;
++}
++
++static void *get_pinned_bpf_obj(const char *pathname){
++	struct inode *inode;
++	struct path path;
++	void *raw;
++	int ret;
++
++	/* Let's get BPF prog 1 */
++	ret = kern_path(pathname, LOOKUP_FOLLOW, &path);
++	if (ret){
++		printk("[syncord] %s failed\n", pathname);
++		return ERR_PTR(ret);
++	}
++
++	inode = d_backing_inode(path.dentry);
++	ret = inode_permission(inode, ACC_MODE(2));
++	if(ret){
++		printk("[syncord] perm error\n");
++		path_put(&path);
++		return ERR_PTR(ret);
++	}
++
++	raw = bpf_any_get(inode->i_private, BPF_TYPE_PROG);
++	if(!IS_ERR(raw)){
++		touch_atime(&path);
++	}
++	else{
++		printk("[syncord] raw error\n");
++		path_put(&path);
++		return ERR_PTR(ret);
++	}
++
++	path_put(&path);
++	return raw;
++}
++
++static int pre_patch_callback(patch_object *obj)
++{
++	extern int num_policy;
++	extern void *bpf_prog_should_reorder[MAX_POLICY];
++
++	if(num_policy < 4)
++		num_policy++;
++	else
++		return -1;
++
++	bpf_prog_should_reorder[num_policy] = get_pinned_bpf_obj("/sys/fs/bpf/numa-grouping");
++	if(IS_ERR(bpf_prog_should_reorder[num_policy])){
++		printk("[syncord] bpf_policy failed\n");
++		return -1;
++	}
++
++	return 0;
++}
++
++static void post_unpatch_callback(patch_object *obj) {
++	extern int num_policy;
++	extern void *bpf_prog_should_reorder[MAX_POLICY];
++
++	bpf_prog_should_reorder[num_policy] = NULL;
++	num_policy--;
++	klp_shadow_free_all(0, NULL);
++}
++KPATCH_PRE_PATCH_CALLBACK(pre_patch_callback);
++KPATCH_POST_UNPATCH_CALLBACK(post_unpatch_callback);
diff -ruN SynCord-linux-base/kernel/kcmp.c SynCord-linux-destination/kernel/kcmp.c
--- SynCord-linux-base/kernel/kcmp.c	2023-08-29 15:29:11.582283462 +0000
+++ SynCord-linux-destination/kernel/kcmp.c	2023-08-31 09:45:11.866140161 +0000
@@ -16,6 +16,7 @@
 #include <linux/list.h>
 #include <linux/eventpoll.h>
 #include <linux/file.h>
+#include <linux/my_bpf_spin_lock.h>
 
 #include <asm/unistd.h>
 
@@ -105,6 +106,7 @@
 			     unsigned long idx1,
 			     struct kcmp_epoll_slot __user *uslot)
 {
+	extern int num_policy;
 	struct file *filp, *filp_epoll, *filp_tgt;
 	struct kcmp_epoll_slot slot;
 	struct files_struct *files;
@@ -120,13 +122,13 @@
 	if (!files)
 		return -EBADF;
 
-	spin_lock(&files->file_lock);
+	my_bpf_spin_lock(&files->file_lock, num_policy);
 	filp_epoll = fcheck_files(files, slot.efd);
 	if (filp_epoll)
 		get_file(filp_epoll);
 	else
 		filp_tgt = ERR_PTR(-EBADF);
-	spin_unlock(&files->file_lock);
+	my_bpf_spin_unlock(&files->file_lock, num_policy);
 	put_files_struct(files);
 
 	if (filp_epoll) {
diff -ruN SynCord-linux-base/kernel/locking/qspinlock.c SynCord-linux-destination/kernel/locking/qspinlock.c
--- SynCord-linux-base/kernel/locking/qspinlock.c	2023-08-29 15:29:11.582283462 +0000
+++ SynCord-linux-destination/kernel/locking/qspinlock.c	2023-08-31 09:43:56.837169532 +0000
@@ -29,6 +29,9 @@
  * Include queued spinlock statistics code
  */
 #include "qspinlock_stat.h"
+#include <linux/lock_policy.h>
+#include <linux/filter.h>
+#include <linux/livepatch.h>
 
 /*
  * The basic principle of a queue-based spinlock can best be understood
@@ -172,6 +175,8 @@
 
 #define MAX_POLICY 5
 int num_policy = 0;
+int num_shuffle = 0;
+int num_notshuffle = 0;
 void *bpf_prog_lock_to_acquire[MAX_POLICY];
 void *bpf_prog_lock_acquired[MAX_POLICY];
 void *bpf_prog_lock_to_release[MAX_POLICY];
@@ -236,7 +241,15 @@
 // Reordering APIs
 static int syncord_should_reorder(struct qspinlock *lock, struct mcs_spinlock *node, struct mcs_spinlock *curr, int policy_id)
 {
-	return 0;
+	struct bpf_prog *prog;
+	prog = bpf_prog_should_reorder[policy_id];
+
+	struct lock_policy_args args;
+	args.numa_node = node->nid;
+	args.next_numa_node = curr->nid;
+
+	int ret = BPF_PROG_RUN(prog, &args);
+	return ret;
 }
 
 static int default_cmp_func(struct qspinlock *lock, struct mcs_spinlock *node, struct mcs_spinlock *curr){
@@ -1018,7 +1031,7 @@
 	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
 		return;
 
-	queued_spin_lock_slowpath(lock, val, 0, 0);
+	queued_spin_lock_slowpath(lock, val, 1, 1);
 }
 EXPORT_SYMBOL(queued_spin_lock);
 
diff -ruN SynCord-linux-base/kernel/locking/qspinlock.c.rej SynCord-linux-destination/kernel/locking/qspinlock.c.rej
--- SynCord-linux-base/kernel/locking/qspinlock.c.rej	1970-01-01 00:00:00.000000000 +0000
+++ SynCord-linux-destination/kernel/locking/qspinlock.c.rej	2023-08-31 09:46:50.649192350 +0000
@@ -0,0 +1,47 @@
+--- kernel/locking/qspinlock.c	2023-08-16 13:14:17.685646927 +0000
++++ kernel/locking/qspinlock.c	2023-08-16 13:18:40.194660869 +0000
+@@ -29,6 +29,9 @@
+  * Include queued spinlock statistics code
+  */
+ #include "qspinlock_stat.h"
++#include <linux/lock_policy.h>
++#include <linux/filter.h>
++#include <linux/livepatch.h>
+ 
+ /*
+  * The basic principle of a queue-based spinlock can best be understood
+@@ -172,6 +175,8 @@
+ 
+ #define MAX_POLICY 5
+ int num_policy = 0;
++int num_shuffle = 0;
++int num_notshuffle = 0;
+ void *bpf_prog_lock_to_acquire[MAX_POLICY];
+ void *bpf_prog_lock_acquired[MAX_POLICY];
+ void *bpf_prog_lock_to_release[MAX_POLICY];
+@@ -236,7 +241,15 @@
+ // Reordering APIs
+ static int syncord_should_reorder(struct qspinlock *lock, struct mcs_spinlock *node, struct mcs_spinlock *curr, int policy_id)
+ {
+-	return 0;
++	struct bpf_prog *prog;
++	prog = bpf_prog_should_reorder[policy_id];
++
++	struct lock_policy_args args;
++	args.numa_node = node->nid;
++	args.next_numa_node = curr->nid;
++
++	int ret = BPF_PROG_RUN(prog, &args);
++	return ret;
+ }
+ 
+ static int default_cmp_func(struct qspinlock *lock, struct mcs_spinlock *node, struct mcs_spinlock *curr){
+@@ -1018,7 +1031,7 @@
+ 	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
+ 		return;
+ 
+-	queued_spin_lock_slowpath(lock, val, 0, 0);
++	queued_spin_lock_slowpath(lock, val, 1, 1);
+ }
+ EXPORT_SYMBOL(queued_spin_lock);
+ 
