diff -ruN SynCord-linux/fs/file.c SynCord-linux-bpf/fs/file.c
--- SynCord-linux/fs/file.c	2023-08-04 08:24:13.852664331 +0000
+++ SynCord-linux-bpf/fs/file.c	2023-08-08 13:30:59.623011150 +0000
@@ -18,6 +18,7 @@
 #include <linux/bitops.h>
 #include <linux/spinlock.h>
 #include <linux/rcupdate.h>
+#include <linux/my_bpf_spin_lock.h>
 
 unsigned int sysctl_nr_open __read_mostly = 1024*1024;
 unsigned int sysctl_nr_open_min = BITS_PER_LONG;
@@ -151,7 +152,8 @@
 {
 	struct fdtable *new_fdt, *cur_fdt;
 
-	spin_unlock(&files->file_lock);
+	extern int num_policy;
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	new_fdt = alloc_fdtable(nr);
 
 	/* make sure all __fd_install() have seen resize_in_progress
@@ -160,7 +162,7 @@
 	if (atomic_read(&files->count) > 1)
 		synchronize_rcu();
 
-	spin_lock(&files->file_lock);
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	if (!new_fdt)
 		return -ENOMEM;
 	/*
@@ -209,10 +211,11 @@
 		return -EMFILE;
 
 	if (unlikely(files->resize_in_progress)) {
-		spin_unlock(&files->file_lock);
+		extern int num_policy;
+                my_bpf_spin_unlock(&files->file_lock,num_policy);
 		expanded = 1;
 		wait_event(files->resize_wait, !files->resize_in_progress);
-		spin_lock(&files->file_lock);
+                my_bpf_spin_lock(&files->file_lock,num_policy);
 		goto repeat;
 	}
 
@@ -271,6 +274,7 @@
  */
 struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 {
+	extern int num_policy;
 	struct files_struct *newf;
 	struct file **old_fds, **new_fds;
 	unsigned int open_files, i;
@@ -294,7 +298,7 @@
 	new_fdt->full_fds_bits = newf->full_fds_bits_init;
 	new_fdt->fd = &newf->fd_array[0];
 
-	spin_lock(&oldf->file_lock);
+        my_bpf_spin_lock(&oldf->file_lock,num_policy);
 	old_fdt = files_fdtable(oldf);
 	open_files = count_open_files(old_fdt);
 
@@ -302,7 +306,7 @@
 	 * Check whether we need to allocate a larger fd array and fd set.
 	 */
 	while (unlikely(open_files > new_fdt->max_fds)) {
-		spin_unlock(&oldf->file_lock);
+                my_bpf_spin_unlock(&oldf->file_lock,num_policy);
 
 		if (new_fdt != &newf->fdtab)
 			__free_fdtable(new_fdt);
@@ -325,7 +329,7 @@
 		 * who knows it may have a new bigger fd table. We need
 		 * the latest pointer.
 		 */
-		spin_lock(&oldf->file_lock);
+                my_bpf_spin_lock(&oldf->file_lock,num_policy);
 		old_fdt = files_fdtable(oldf);
 		open_files = count_open_files(old_fdt);
 	}
@@ -350,7 +354,7 @@
 		}
 		rcu_assign_pointer(*new_fds++, f);
 	}
-	spin_unlock(&oldf->file_lock);
+        my_bpf_spin_unlock(&oldf->file_lock,num_policy);
 
 	/* clear the remainder */
 	memset(new_fds, 0, (new_fdt->max_fds - open_files) * sizeof(struct file *));
@@ -484,7 +488,8 @@
 	int error;
 	struct fdtable *fdt;
 
-	spin_lock(&files->file_lock);
+	extern int num_policy;
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 repeat:
 	fdt = files_fdtable(files);
 	fd = start;
@@ -531,7 +536,7 @@
 #endif
 
 out:
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	return error;
 }
 
@@ -557,9 +562,10 @@
 void put_unused_fd(unsigned int fd)
 {
 	struct files_struct *files = current->files;
-	spin_lock(&files->file_lock);
+	extern int num_policy;
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	__put_unused_fd(files, fd);
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 }
 
 EXPORT_SYMBOL(put_unused_fd);
@@ -587,17 +593,18 @@
 void __fd_install(struct files_struct *files, unsigned int fd,
 		struct file *file)
 {
+	extern int num_policy;
 	struct fdtable *fdt;
 
 	rcu_read_lock_sched();
 
 	if (unlikely(files->resize_in_progress)) {
 		rcu_read_unlock_sched();
-		spin_lock(&files->file_lock);
+        	my_bpf_spin_lock(&files->file_lock,num_policy);
 		fdt = files_fdtable(files);
 		BUG_ON(fdt->fd[fd] != NULL);
 		rcu_assign_pointer(fdt->fd[fd], file);
-		spin_unlock(&files->file_lock);
+        	my_bpf_spin_unlock(&files->file_lock,num_policy);
 		return;
 	}
 	/* coupled with smp_wmb() in expand_fdtable() */
@@ -623,7 +630,8 @@
 	struct file *file;
 	struct fdtable *fdt;
 
-	spin_lock(&files->file_lock);
+	extern int num_policy;
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	fdt = files_fdtable(files);
 	if (fd >= fdt->max_fds)
 		goto out_unlock;
@@ -632,11 +640,11 @@
 		goto out_unlock;
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	__put_unused_fd(files, fd);
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	return filp_close(file, files);
 
 out_unlock:
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	return -EBADF;
 }
 EXPORT_SYMBOL(__close_fd); /* for ksys_close() */
@@ -650,7 +658,8 @@
 	struct file *file;
 	struct fdtable *fdt;
 
-	spin_lock(&files->file_lock);
+	extern int num_policy;
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	fdt = files_fdtable(files);
 	if (fd >= fdt->max_fds)
 		goto out_unlock;
@@ -659,13 +668,13 @@
 		goto out_unlock;
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	__put_unused_fd(files, fd);
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	get_file(file);
 	*res = file;
 	return filp_close(file, files);
 
 out_unlock:
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	*res = NULL;
 	return -ENOENT;
 }
@@ -676,7 +685,8 @@
 	struct fdtable *fdt;
 
 	/* exec unshares first */
-	spin_lock(&files->file_lock);
+	extern int num_policy;
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	for (i = 0; ; i++) {
 		unsigned long set;
 		unsigned fd = i * BITS_PER_LONG;
@@ -696,14 +706,14 @@
 				continue;
 			rcu_assign_pointer(fdt->fd[fd], NULL);
 			__put_unused_fd(files, fd);
-			spin_unlock(&files->file_lock);
+        		my_bpf_spin_unlock(&files->file_lock,num_policy);
 			filp_close(file, files);
 			cond_resched();
-			spin_lock(&files->file_lock);
+        		my_bpf_spin_lock(&files->file_lock,num_policy);
 		}
 
 	}
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 }
 
 static struct file *__fget(unsigned int fd, fmode_t mask, unsigned int refs)
@@ -819,13 +829,14 @@
 {
 	struct files_struct *files = current->files;
 	struct fdtable *fdt;
-	spin_lock(&files->file_lock);
+	extern int num_policy;
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	fdt = files_fdtable(files);
 	if (flag)
 		__set_close_on_exec(fd, fdt);
 	else
 		__clear_close_on_exec(fd, fdt);
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 }
 
 bool get_close_on_exec(unsigned int fd)
@@ -844,6 +855,7 @@
 	struct file *file, unsigned fd, unsigned flags)
 __releases(&files->file_lock)
 {
+	extern int num_policy;
 	struct file *tofree;
 	struct fdtable *fdt;
 
@@ -872,7 +884,7 @@
 		__set_close_on_exec(fd, fdt);
 	else
 		__clear_close_on_exec(fd, fdt);
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 
 	if (tofree)
 		filp_close(tofree, files);
@@ -880,12 +892,13 @@
 	return fd;
 
 Ebusy:
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	return -EBUSY;
 }
 
 int replace_fd(unsigned fd, struct file *file, unsigned flags)
 {
+	extern int num_policy;
 	int err;
 	struct files_struct *files = current->files;
 
@@ -895,19 +908,20 @@
 	if (fd >= rlimit(RLIMIT_NOFILE))
 		return -EBADF;
 
-	spin_lock(&files->file_lock);
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	err = expand_files(files, fd);
 	if (unlikely(err < 0))
 		goto out_unlock;
 	return do_dup2(files, file, fd, flags);
 
 out_unlock:
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	return err;
 }
 
 static int ksys_dup3(unsigned int oldfd, unsigned int newfd, int flags)
 {
+	extern int num_policy;
 	int err = -EBADF;
 	struct file *file;
 	struct files_struct *files = current->files;
@@ -921,7 +935,7 @@
 	if (newfd >= rlimit(RLIMIT_NOFILE))
 		return -EBADF;
 
-	spin_lock(&files->file_lock);
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	err = expand_files(files, newfd);
 	file = fcheck(oldfd);
 	if (unlikely(!file))
@@ -936,7 +950,7 @@
 Ebadf:
 	err = -EBADF;
 out_unlock:
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	return err;
 }
 
@@ -997,11 +1011,12 @@
 		int (*f)(const void *, struct file *, unsigned),
 		const void *p)
 {
+	extern int num_policy;
 	struct fdtable *fdt;
 	int res = 0;
 	if (!files)
 		return 0;
-	spin_lock(&files->file_lock);
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	for (fdt = files_fdtable(files); n < fdt->max_fds; n++) {
 		struct file *file;
 		file = rcu_dereference_check_fdtable(files, fdt->fd[n]);
@@ -1011,7 +1026,7 @@
 		if (res)
 			break;
 	}
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	return res;
 }
 EXPORT_SYMBOL(iterate_fd);
diff -ruN SynCord-linux/fs/locks.c SynCord-linux-bpf/fs/locks.c
--- SynCord-linux/fs/locks.c	2023-08-04 08:24:13.872664719 +0000
+++ SynCord-linux-bpf/fs/locks.c	2023-08-08 12:44:24.322898093 +0000
@@ -168,6 +168,8 @@
 #include <linux/pid_namespace.h>
 #include <linux/hashtable.h>
 #include <linux/percpu.h>
+#include <linux/my_bpf_spin_lock.h>
+
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/filelock.h>
@@ -2509,9 +2511,10 @@
 		 * update of i_flctx->flc_posix and check for it done in
 		 * close(). rcu_read_lock() wouldn't do.
 		 */
-		spin_lock(&current->files->file_lock);
+		extern int num_policy;
+		my_bpf_spin_lock(&current->files->file_lock,num_policy);
 		f = fcheck(fd);
-		spin_unlock(&current->files->file_lock);
+		my_bpf_spin_unlock(&current->files->file_lock,num_policy);
 		if (f != filp) {
 			file_lock->fl_type = F_UNLCK;
 			error = do_lock_file_wait(filp, cmd, file_lock);
@@ -2640,9 +2643,10 @@
 		 * update of i_flctx->flc_posix and check for it done in
 		 * close(). rcu_read_lock() wouldn't do.
 		 */
-		spin_lock(&current->files->file_lock);
+		extern int num_policy;
+        	my_bpf_spin_lock(&current->files->file_lock,num_policy);
 		f = fcheck(fd);
-		spin_unlock(&current->files->file_lock);
+        	my_bpf_spin_unlock(&current->files->file_lock,num_policy);
 		if (f != filp) {
 			file_lock->fl_type = F_UNLCK;
 			error = do_lock_file_wait(filp, cmd, file_lock);
diff -ruN SynCord-linux/fs/proc/fd.c SynCord-linux-bpf/fs/proc/fd.c
--- SynCord-linux/fs/proc/fd.c	2023-08-04 08:24:13.920665648 +0000
+++ SynCord-linux-bpf/fs/proc/fd.c	2023-08-08 12:11:58.146207099 +0000
@@ -10,6 +10,7 @@
 #include <linux/file.h>
 #include <linux/seq_file.h>
 #include <linux/fs.h>
+#include <linux/my_bpf_spin_lock.h>
 
 #include <linux/proc_fs.h>
 
@@ -34,7 +35,8 @@
 	if (files) {
 		unsigned int fd = proc_fd(m->private);
 
-		spin_lock(&files->file_lock);
+		extern int num_policy;
+                my_bpf_spin_lock(&files->file_lock,num_policy);
 		file = fcheck_files(files, fd);
 		if (file) {
 			struct fdtable *fdt = files_fdtable(files);
@@ -46,7 +48,7 @@
 			get_file(file);
 			ret = 0;
 		}
-		spin_unlock(&files->file_lock);
+                my_bpf_spin_unlock(&files->file_lock,num_policy);
 		put_files_struct(files);
 	}
 
@@ -160,13 +162,15 @@
 		unsigned int fd = proc_fd(d_inode(dentry));
 		struct file *fd_file;
 
-		spin_lock(&files->file_lock);
+		extern int num_policy;
+                my_bpf_spin_lock(&files->file_lock,num_policy);
 		fd_file = fcheck_files(files, fd);
 		if (fd_file) {
 			*path = fd_file->f_path;
 			path_get(&fd_file->f_path);
 			ret = 0;
 		}
+                my_bpf_spin_unlock(&files->file_lock,num_policy);
 		spin_unlock(&files->file_lock);
 		put_files_struct(files);
 	}
Binary files SynCord-linux/.git/index and SynCord-linux-bpf/.git/index differ
diff -ruN SynCord-linux/include/linux/lock_policy.h SynCord-linux-bpf/include/linux/lock_policy.h
--- SynCord-linux/include/linux/lock_policy.h	2023-08-08 14:49:22.054351205 +0000
+++ SynCord-linux-bpf/include/linux/lock_policy.h	2023-08-07 08:42:06.897380668 +0000
@@ -19,9 +19,9 @@
 	unsigned long arg3;
 	unsigned long arg4;
 	unsigned long numa_node;
-        unsigned long next_numa_node;
-        unsigned long arg7;
-        unsigned long arg8;
+	unsigned long next_numa_node;
+	unsigned long arg7;
+	unsigned long arg8;
 };
 
 
@@ -39,9 +39,9 @@
 	unsigned long *arg3;
 	unsigned long *arg4;
 	unsigned long numa_node;
-        unsigned long next_numa_node;
+	unsigned long next_numa_node;
 	unsigned long arg7;
-        unsigned long arg8;
+	unsigned long arg8;
 
 	unsigned long tmp_reg;
 };
@@ -52,7 +52,7 @@
  */
 struct per_lock_data {
 	/* User-defined additional data */
-	struct __aligned_u64_field per_cpu_data[224];
+	struct __aligned_u64_field per_cpu_data[NR_CPUS];
 };
 
 /*
diff -ruN SynCord-linux/include/linux/my_bpf_spin_lock.h SynCord-linux-bpf/include/linux/my_bpf_spin_lock.h
--- SynCord-linux/include/linux/my_bpf_spin_lock.h	1970-01-01 00:00:00.000000000 +0000
+++ SynCord-linux-bpf/include/linux/my_bpf_spin_lock.h	2023-08-08 13:55:50.001943858 +0000
@@ -0,0 +1,68 @@
+#ifndef MY_BPF_SPIN_H
+#define MY_BPF_SPIN_H
+
+#include <linux/kernel.h>
+extern inline void my_bpf_spin_lock(spinlock_t *lock,int policy);
+extern inline void my_bpf_spin_unlock(spinlock_t *lock,int policy);
+static inline void bpf___raw_spin_lock(raw_spinlock_t *lock,int policy);
+static inline void bpf___raw_spin_unlock(raw_spinlock_t *lock,int policy);
+static inline void bpf_do_raw_spin_lock(raw_spinlock_t *lock, int policy) __acquires(lock);
+static inline void bpf_do_raw_spin_unlock(raw_spinlock_t *lock, int policy) __releases(lock);
+static void bpf_arch_spin_lock(struct qspinlock *lock, int policy);
+static void bpf_arch_spin_unlock(struct qspinlock *lock, int policy);
+
+extern inline void my_bpf_spin_lock(spinlock_t *lock,int policy)
+{
+        bpf___raw_spin_lock(&lock->rlock,policy);
+	printk(KERN_INFO "my_bpf_spin_lock");
+}
+
+extern inline void my_bpf_spin_unlock(spinlock_t *lock,int policy)
+{
+        bpf___raw_spin_unlock(&lock->rlock,policy);
+	printk(KERN_INFO "my_bpf_spin_unlock");
+}
+
+
+static inline void bpf___raw_spin_lock(raw_spinlock_t *lock,int policy)
+{
+        preempt_disable();
+        spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+        bpf_do_raw_spin_lock(lock,policy);
+	printk(KERN_INFO "bpf___raw_spin_lock");
+}
+
+static inline void bpf___raw_spin_unlock(raw_spinlock_t *lock,int policy)
+{
+        spin_release(&lock->dep_map, 1, _RET_IP_);
+        bpf_do_raw_spin_unlock(lock,policy);
+        preempt_enable();
+}
+
+static inline void bpf_do_raw_spin_lock(raw_spinlock_t *lock, int policy) __acquires(lock)
+{
+        __acquire(lock);
+        bpf_arch_spin_lock(&lock->raw_lock,policy);
+        mmiowb_spin_lock();
+}
+
+
+static inline void bpf_do_raw_spin_unlock(raw_spinlock_t *lock, int policy) __releases(lock)
+{
+        mmiowb_spin_unlock();
+        bpf_arch_spin_unlock(&lock->raw_lock,policy);
+        __release(lock);
+}
+
+
+static void bpf_arch_spin_lock(struct qspinlock *lock, int policy)
+{
+       bpf_queued_spin_lock(lock,policy);
+}
+
+static void bpf_arch_spin_unlock(struct qspinlock *lock, int policy)
+{
+        bpf_queued_spin_unlock(lock,policy);
+}
+
+#endif 
diff -ruN SynCord-linux/kernel/bpf/inode.c SynCord-linux-bpf/kernel/bpf/inode.c
--- SynCord-linux/kernel/bpf/inode.c	2023-08-04 08:24:14.172670548 +0000
+++ SynCord-linux-bpf/kernel/bpf/inode.c	2023-08-09 08:50:27.169532370 +0000
@@ -701,3 +701,76 @@
 	return ret;
 }
 fs_initcall(bpf_init);
+
+#include "kpatch-macros.h"
+#define MAX_POLICY 5
+
+static inline __u64 ptr_to_u64(const void *ptr)
+{
+    return (__u64) (unsigned long) ptr;
+}
+
+static void *get_pinned_bpf_obj(const char *pathname){
+	struct inode *inode;
+	struct path path;
+	void *raw;
+	int ret;
+
+	/* Let's get BPF prog 1 */
+	ret = kern_path(pathname, LOOKUP_FOLLOW, &path);
+	if (ret){
+		printk("[syncord] %s failed\n", pathname);
+		return ERR_PTR(ret);
+	}
+
+	inode = d_backing_inode(path.dentry);
+	ret = inode_permission(inode, ACC_MODE(2));
+	if(ret){
+		printk("[syncord] perm error\n");
+		path_put(&path);
+		return ERR_PTR(ret);
+	}
+
+	raw = bpf_any_get(inode->i_private, BPF_TYPE_PROG);
+	if(!IS_ERR(raw)){
+		touch_atime(&path);
+	}
+	else{
+		printk("[syncord] raw error\n");
+		path_put(&path);
+		return ERR_PTR(ret);
+	}
+
+	path_put(&path);
+	return raw;
+}
+
+static int pre_patch_callback(patch_object *obj)
+{
+	extern int num_policy;
+	extern void *bpf_prog_should_reorder[MAX_POLICY];
+
+	if(num_policy < 4)
+		num_policy++;
+	else
+		return -1;
+
+	bpf_prog_should_reorder[num_policy] = get_pinned_bpf_obj("/sys/fs/bpf/numa-grouping");
+	if(IS_ERR(bpf_prog_should_reorder[num_policy])){
+		printk("[syncord] bpf_policy failed\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+static void post_unpatch_callback(patch_object *obj) {
+	extern int num_policy;
+	extern void *bpf_prog_should_reorder[MAX_POLICY];
+
+	bpf_prog_should_reorder[num_policy] = NULL;
+	num_policy--;
+	klp_shadow_free_all(0, NULL);
+}
+KPATCH_PRE_PATCH_CALLBACK(pre_patch_callback);
+KPATCH_POST_UNPATCH_CALLBACK(post_unpatch_callback);
diff -ruN SynCord-linux/kernel/bpf/syscall.c SynCord-linux-bpf/kernel/bpf/syscall.c
--- SynCord-linux/kernel/bpf/syscall.c	2023-08-04 08:24:14.176670625 +0000
+++ SynCord-linux-bpf/kernel/bpf/syscall.c	2023-08-08 13:50:02.717503901 +0000
@@ -23,6 +23,7 @@
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
 #include <linux/nospec.h>
+#include <linux/my_bpf_spin_lock.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
 			   (map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
@@ -2788,13 +2789,13 @@
 		return -ENOENT;
 
 	err = 0;
-	spin_lock(&files->file_lock);
+        spin_lock(&files->file_lock);
 	file = fcheck_files(files, fd);
 	if (!file)
 		err = -EBADF;
 	else
 		get_file(file);
-	spin_unlock(&files->file_lock);
+        spin_unlock(&files->file_lock);
 	put_files_struct(files);
 
 	if (err)
diff -ruN SynCord-linux/kernel/kcmp.c SynCord-linux-bpf/kernel/kcmp.c
--- SynCord-linux/kernel/kcmp.c	2023-08-04 08:24:14.184670781 +0000
+++ SynCord-linux-bpf/kernel/kcmp.c	2023-08-08 13:31:28.074998956 +0000
@@ -16,6 +16,7 @@
 #include <linux/list.h>
 #include <linux/eventpoll.h>
 #include <linux/file.h>
+#include <linux/my_bpf_spin_lock.h>
 
 #include <asm/unistd.h>
 
@@ -105,6 +106,7 @@
 			     unsigned long idx1,
 			     struct kcmp_epoll_slot __user *uslot)
 {
+	extern int num_policy;
 	struct file *filp, *filp_epoll, *filp_tgt;
 	struct kcmp_epoll_slot slot;
 	struct files_struct *files;
@@ -120,13 +122,13 @@
 	if (!files)
 		return -EBADF;
 
-	spin_lock(&files->file_lock);
+        my_bpf_spin_lock(&files->file_lock,num_policy);
 	filp_epoll = fcheck_files(files, slot.efd);
 	if (filp_epoll)
 		get_file(filp_epoll);
 	else
 		filp_tgt = ERR_PTR(-EBADF);
-	spin_unlock(&files->file_lock);
+        my_bpf_spin_unlock(&files->file_lock,num_policy);
 	put_files_struct(files);
 
 	if (filp_epoll) {
diff -ruN SynCord-linux/kernel/locking/spinlock.c SynCord-linux-bpf/kernel/locking/spinlock.c
--- SynCord-linux/kernel/locking/spinlock.c	2023-08-04 08:24:14.188670859 +0000
+++ SynCord-linux-bpf/kernel/locking/spinlock.c	2023-08-07 09:57:10.320818181 +0000
@@ -397,3 +397,4 @@
 	&& addr < (unsigned long)__lock_text_end;
 }
 EXPORT_SYMBOL(in_lock_functions);
+
